{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ez4CZaGutjJ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.transforms.functional as TF\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CityscapesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, split='train', mode='fine', augment=False):\n",
        "\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.mode = 'gtFine' if mode == 'fine' else 'gtCoarse'\n",
        "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
        "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
        "        self.split = split\n",
        "        self.augment = augment\n",
        "        self.images = []\n",
        "        self.targets = []\n",
        "        self.mapping = {\n",
        "            0: 0,  # unlabeled\n",
        "            1: 0,  # ego vehicle\n",
        "            2: 0,  # rect border\n",
        "            3: 0,  # out of roi\n",
        "            4: 0,  # static\n",
        "            5: 0,  # dynamic\n",
        "            6: 0,  # ground\n",
        "            7: 1,  # road\n",
        "            8: 0,  # sidewalk\n",
        "            9: 0,  # parking\n",
        "            10: 0,  # rail track\n",
        "            11: 0,  # building\n",
        "            12: 0,  # wall\n",
        "            13: 0,  # fence\n",
        "            14: 0,  # guard rail\n",
        "            15: 0,  # bridge\n",
        "            16: 0,  # tunnel\n",
        "            17: 0,  # pole\n",
        "            18: 0,  # polegroup\n",
        "            19: 0,  # traffic light\n",
        "            20: 0,  # traffic sign\n",
        "            21: 0,  # vegetation\n",
        "            22: 0,  # terrain\n",
        "            23: 2,  # sky\n",
        "            24: 0,  # person\n",
        "            25: 0,  # rider\n",
        "            26: 3,  # car\n",
        "            27: 0,  # truck\n",
        "            28: 0,  # bus\n",
        "            29: 0,  # caravan\n",
        "            30: 0,  # trailer\n",
        "            31: 0,  # train\n",
        "            32: 0,  # motorcycle\n",
        "            33: 0,  # bicycle\n",
        "            -1: 0  # licenseplate\n",
        "        }\n",
        "        self.mappingrgb = {\n",
        "            0: (255, 0, 0),  # unlabeled\n",
        "            1: (255, 0, 0),  # ego vehicle\n",
        "            2: (255, 0, 0),  # rect border\n",
        "            3: (255, 0, 0),  # out of roi\n",
        "            4: (255, 0, 0),  # static\n",
        "            5: (255, 0, 0),  # dynamic\n",
        "            6: (255, 0, 0),  # ground\n",
        "            7: (0, 255, 0),  # road\n",
        "            8: (255, 0, 0),  # sidewalk\n",
        "            9: (255, 0, 0),  # parking\n",
        "            10: (255, 0, 0),  # rail track\n",
        "            11: (255, 0, 0),  # building\n",
        "            12: (255, 0, 0),  # wall\n",
        "            13: (255, 0, 0),  # fence\n",
        "            14: (255, 0, 0),  # guard rail\n",
        "            15: (255, 0, 0),  # bridge\n",
        "            16: (255, 0, 0),  # tunnel\n",
        "            17: (255, 0, 0),  # pole\n",
        "            18: (255, 0, 0),  # polegroup\n",
        "            19: (255, 0, 0),  # traffic light\n",
        "            20: (255, 0, 0),  # traffic sign\n",
        "            21: (255, 0, 0),  # vegetation\n",
        "            22: (255, 0, 0),  # terrain\n",
        "            23: (0, 0, 255),  # sky\n",
        "            24: (255, 0, 0),  # person\n",
        "            25: (255, 0, 0),  # rider\n",
        "            26: (255, 255, 0),  # car\n",
        "            27: (255, 0, 0),  # truck\n",
        "            28: (255, 0, 0),  # bus\n",
        "            29: (255, 0, 0),  # caravan\n",
        "            30: (255, 0, 0),  # trailer\n",
        "            31: (255, 0, 0),  # train\n",
        "            32: (255, 0, 0),  # motorcycle\n",
        "            33: (255, 0, 0),  # bicycle\n",
        "            -1: (255, 0, 0)  # licenseplate\n",
        "        }\n",
        "\n",
        "        # Ensure that this matches the above mapping!#!@#!@#\n",
        "        # For example 4 classes, means we should map to the ids=(0,1,2,3)\n",
        "        # This is used to specify how many outputs the network should product...\n",
        "        self.num_classes = 4\n",
        "\n",
        "        # =============================================\n",
        "        # Check that inputs are valid\n",
        "        # =============================================\n",
        "        if mode not in ['fine', 'coarse']:\n",
        "            raise ValueError('Invalid mode! Please use mode=\"fine\" or mode=\"coarse\"')\n",
        "        if mode == 'fine' and split not in ['train', 'test', 'val']:\n",
        "            raise ValueError('Invalid split for mode \"fine\"! Please use split=\"train\", split=\"test\" or split=\"val\"')\n",
        "        elif mode == 'coarse' and split not in ['train', 'train_extra', 'val']:\n",
        "            raise ValueError('Invalid split for mode \"coarse\"! Please use split=\"train\", split=\"train_extra\" or split=\"val\"')\n",
        "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
        "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
        "                               ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
        "\n",
        "        # =============================================\n",
        "        # Read in the paths to all images\n",
        "        # =============================================\n",
        "        for city in os.listdir(self.images_dir):\n",
        "            img_dir = os.path.join(self.images_dir, city)\n",
        "            target_dir = os.path.join(self.targets_dir, city)\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                self.images.append(os.path.join(img_dir, file_name))\n",
        "                target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0], '{}_labelIds.png'.format(self.mode))\n",
        "                # target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0], '{}_color.png'.format(self.mode))\n",
        "                self.targets.append(os.path.join(target_dir, target_name))\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of images: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Split: {}\\n'.format(self.split)\n",
        "        fmt_str += '    Mode: {}\\n'.format(self.mode)\n",
        "        fmt_str += '    Augment: {}\\n'.format(self.augment)\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        return fmt_str\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def mask_to_class(self, mask):\n",
        "        '''\n",
        "        Given the cityscapes dataset, this maps to a 0..classes numbers.\n",
        "        This is because we are using a subset of all masks, so we have this \"mapping\" function.\n",
        "        This mapping function is used to map all the standard ids into the smaller subset.\n",
        "        '''\n",
        "        maskimg = torch.zeros((mask.size()[0], mask.size()[1]), dtype=torch.uint8)\n",
        "        for k in self.mapping:\n",
        "            maskimg[mask == k] = self.mapping[k]\n",
        "        return maskimg\n",
        "\n",
        "    def mask_to_rgb(self, mask):\n",
        "        '''\n",
        "        Given the Cityscapes mask file, this converts the ids into rgb colors.\n",
        "        This is needed as we are interested in a sub-set of labels, thus can't just use the\n",
        "        standard color output provided by the dataset.\n",
        "        '''\n",
        "        rgbimg = torch.zeros((3, mask.size()[0], mask.size()[1]), dtype=torch.uint8)\n",
        "        for k in self.mappingrgb:\n",
        "            rgbimg[0][mask == k] = self.mappingrgb[k][0]\n",
        "            rgbimg[1][mask == k] = self.mappingrgb[k][1]\n",
        "            rgbimg[2][mask == k] = self.mappingrgb[k][2]\n",
        "        return rgbimg\n",
        "\n",
        "    def class_to_rgb(self, mask):\n",
        "        '''\n",
        "        This function maps the classification index ids into the rgb.\n",
        "        For example after the argmax from the network, you want to find what class\n",
        "        a given pixel belongs too. This does that but just changes the color\n",
        "        so that we can compare it directly to the rgb groundtruth label.\n",
        "        '''\n",
        "        mask2class = dict((v, k) for k, v in self.mapping.items())\n",
        "        rgbimg = torch.zeros((3, mask.size()[0], mask.size()[1]), dtype=torch.uint8)\n",
        "        for k in mask2class:\n",
        "            rgbimg[0][mask == k] = self.mappingrgb[mask2class[k]][0]\n",
        "            rgbimg[1][mask == k] = self.mappingrgb[mask2class[k]][1]\n",
        "            rgbimg[2][mask == k] = self.mappingrgb[mask2class[k]][2]\n",
        "        return rgbimg\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # first load the RGB image\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "\n",
        "        # next load the target\n",
        "        target = Image.open(self.targets[index]).convert('L')\n",
        "\n",
        "        # If augmenting, apply random transforms\n",
        "        # Else we should just resize the image down to the correct size\n",
        "        if self.augment:\n",
        "            # Resize\n",
        "            image = TF.resize(image, size=(128+10, 256+10), interpolation=Image.BILINEAR)\n",
        "            target = TF.resize(target, size=(128+10, 256+10), interpolation=Image.NEAREST)\n",
        "            # Random crop\n",
        "            i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(128, 256))\n",
        "            image = TF.crop(image, i, j, h, w)\n",
        "            target = TF.crop(target, i, j, h, w)\n",
        "            # Random horizontal flipping\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.hflip(image)\n",
        "                target = TF.hflip(target)\n",
        "            # Random vertical flipping\n",
        "            # (I found this caused issues with the sky=road during prediction)\n",
        "            # if random.random() > 0.5:\n",
        "            #    image = TF.vflip(image)\n",
        "            #    target = TF.vflip(target)\n",
        "        else:\n",
        "            # Resize\n",
        "            image = TF.resize(image, size=(128, 256), interpolation=Image.BILINEAR)\n",
        "            target = TF.resize(target, size=(128, 256), interpolation=Image.NEAREST)\n",
        "\n",
        "        # convert to pytorch tensors\n",
        "        # target = TF.to_tensor(target)\n",
        "        target = torch.from_numpy(np.array(target, dtype=np.uint8))\n",
        "        image = TF.to_tensor(image)\n",
        "\n",
        "        # convert the labels into a mask\n",
        "        targetrgb = self.mask_to_rgb(target)\n",
        "        targetmask = self.mask_to_class(target)\n",
        "        targetmask = targetmask.long()\n",
        "        targetrgb = targetrgb.long()\n",
        "\n",
        "        # finally return the image pair\n",
        "        return image, targetmask, targetrgb"
      ],
      "metadata": {
        "id": "ouqt6hgItuyn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def conv_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_trans_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def maxpool():\n",
        "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    return pool\n",
        "\n",
        "\n",
        "def conv_block_2(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_block_3(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        conv_block(out_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "2JZQvKxltwDP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, num_filter):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_filter = num_filter\n",
        "        act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.down_1 = conv_block_2(self.in_dim, self.num_filter, act_fn)\n",
        "        self.pool_1 = maxpool()\n",
        "        self.down_2 = conv_block_2(self.num_filter*1, self.num_filter*2, act_fn)\n",
        "        self.pool_2 = maxpool()\n",
        "        self.down_3 = conv_block_2(self.num_filter*2, self.num_filter*4, act_fn)\n",
        "        self.pool_3 = maxpool()\n",
        "        self.down_4 = conv_block_2(self.num_filter*4, self.num_filter*8, act_fn)\n",
        "        self.pool_4 = maxpool()\n",
        "\n",
        "        self.bridge = conv_block_2(self.num_filter*8, self.num_filter*16, act_fn)\n",
        "\n",
        "        self.trans_1 = conv_trans_block(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "        self.up_1 = conv_block_2(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "        self.trans_2 = conv_trans_block(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "        self.up_2 = conv_block_2(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "        self.trans_3 = conv_trans_block(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "        self.up_3 = conv_block_2(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "        self.trans_4 = conv_trans_block(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "        self.up_4 = conv_block_2(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(self.num_filter, self.out_dim, 3, 1, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        down_1 = self.down_1(input)\n",
        "        pool_1 = self.pool_1(down_1)\n",
        "        down_2 = self.down_2(pool_1)\n",
        "        pool_2 = self.pool_2(down_2)\n",
        "        down_3 = self.down_3(pool_2)\n",
        "        pool_3 = self.pool_3(down_3)\n",
        "        down_4 = self.down_4(pool_3)\n",
        "        pool_4 = self.pool_4(down_4)\n",
        "\n",
        "        bridge = self.bridge(pool_4)\n",
        "\n",
        "        trans_1 = self.trans_1(bridge)\n",
        "        concat_1 = torch.cat([trans_1, down_4], dim=1)\n",
        "        up_1 = self.up_1(concat_1)\n",
        "        trans_2 = self.trans_2(up_1)\n",
        "        concat_2 = torch.cat([trans_2, down_3], dim=1)\n",
        "        up_2 = self.up_2(concat_2)\n",
        "        trans_3 = self.trans_3(up_2)\n",
        "        concat_3 = torch.cat([trans_3, down_2], dim=1)\n",
        "        up_3 = self.up_3(concat_3)\n",
        "        trans_4 = self.trans_4(up_3)\n",
        "        concat_4 = torch.cat([trans_4, down_1], dim=1)\n",
        "        up_4 = self.up_4(concat_4)\n",
        "\n",
        "        out = self.out(up_4)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9p4q5Q-dtwTv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lr4bAxfitwiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzD53vustz22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVfiGaFkt0Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ej-sHfnmt0eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jwzw32G1t0sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXIEbz80t07F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIyrWDc7t1I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6fNsmG6mt1Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyroRWBAt1iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjE8ofkIt1vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UT8v7nq4t18P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RIgG9cWt2JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEM7Aj3At2U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZRy3wI1t2gU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}